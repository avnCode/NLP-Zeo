{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gQ6jso-qMEmA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#undersampling"
      ],
      "metadata": {
        "id": "ylLeXDG4-D_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples = 1381 # we have these many SCIENCE articles and SCIENCE is our minority class\n",
        "df_business = df[df.category==\"BUSINESS\"].sample(min_samples, random_state=2022)\n",
        "df_sports = df[df.category==\"SPORTS\"].sample(min_samples, random_state=2022)\n",
        "df_crime = df[df.category==\"CRIME\"].sample(min_samples, random_state=2022)\n",
        "df_science = df[df.category==\"SCIENCE\"].sample(min_samples, random_state=2022)\n",
        "df_balanced = pd.concat([df_business,df_sports,df_crime,df_science],axis=0)\n",
        "df_balanced.category.value_counts()"
      ],
      "metadata": {
        "id": "ggCUctgN-C_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BoW n-grams"
      ],
      "metadata": {
        "id": "gXjHmol6874U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "v = CountVectorizer(ngram_range=(1,2))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "id": "V0ByeZsJ8_Fr",
        "outputId": "714273de-b8fe-4510-a4fc-eddaba7cda2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 9,\n",
              " 'hathodawala': 2,\n",
              " 'is': 4,\n",
              " 'looking': 7,\n",
              " 'for': 0,\n",
              " 'job': 6,\n",
              " 'thor hathodawala': 10,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 5,\n",
              " 'looking for': 8,\n",
              " 'for job': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v.transform([\"Thor eat pizza is looking\"]).toarray()"
      ],
      "metadata": {
        "id": "OYD10c4y9iMB",
        "outputId": "2b4177d3-8d5f-429f-b22f-6a59c4801d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('news_dataset.json')\n",
        "df.category.value_counts()\n"
      ],
      "metadata": {
        "id": "GMRee7SW96yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples = 1381 # we have these many SCIENCE articles and SCIENCE is our minority class\n",
        "df_business = df[df.category==\"BUSINESS\"].sample(min_samples, random_state=2022)\n",
        "df_sports = df[df.category==\"SPORTS\"].sample(min_samples, random_state=2022)\n",
        "df_crime = df[df.category==\"CRIME\"].sample(min_samples, random_state=2022)\n",
        "df_science = df[df.category==\"SCIENCE\"].sample(min_samples, random_state=2022)\n",
        "df_balanced = pd.concat([df_business,df_sports,df_crime,df_science],axis=0)\n",
        "df_balanced.category.value_counts()"
      ],
      "metadata": {
        "id": "e0M_VddZ-IB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = {'BUSINESS': 0, 'SPORTS': 1, 'CRIME': 2, 'SCIENCE': 3}\n",
        "\n",
        "df_balanced['category_num'] = df_balanced['category'].map({\n",
        "    'BUSINESS': 0,\n",
        "    'SPORTS': 1,\n",
        "    'CRIME': 2,\n",
        "    'SCIENCE': 3\n",
        "})"
      ],
      "metadata": {
        "id": "iA05bnNc-QBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_balanced.text,\n",
        "    df_balanced.category_num,\n",
        "    test_size=0.2, # 20% samples will go to test dataset\n",
        "    random_state=2022,\n",
        "    stratify=df_balanced.category_num\n",
        ")"
      ],
      "metadata": {
        "id": "mB4sejRa-Y35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "clf = Pipeline([\n",
        "     ('vectorizer_bow', CountVectorizer(ngram_range = (1, 1))),        #using the ngram_range parameter\n",
        "     ('Multi NB', MultinomialNB())\n",
        "])\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ohAvoUJz-ewM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "hjVzm3x9Qjao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/Ecommerce_data.csv\")"
      ],
      "metadata": {
        "id": "rmRiWYEDVv89"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Bzznnx_hbJ-6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "df['label'].value_counts()\n",
        "df['target'] = df.label.map({\"Household\":0,\"Electronics\":1,\"Clothing & Accessories\":2,\"Books\":3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2SK8rJMV5J1",
        "outputId": "828b71d6-8a04-4463-f0a2-553695726cef"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Text', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only Preprocess\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess(text):\n",
        "  doc = nlp(text)\n",
        "  fil_token = []\n",
        "  for token in doc:\n",
        "    if token.is_punct or token.is_stop:\n",
        "      continue\n",
        "    else:\n",
        "      fil_token.append(token.lemma_)\n",
        "  return \" \".join(fil_token)"
      ],
      "metadata": {
        "id": "JGhukjx0V-VB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Text = df.Text.apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "id": "I6_iu3c-Xl-B"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.target, test_size = 0.2, stratify=df.label)"
      ],
      "metadata": {
        "id": "a-TbZQLVX4w8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = Pipeline([\n",
        "     ('vectorizer_tfidf',TfidfVectorizer()),        #using the ngram_range parameter\n",
        "     ('Random Forest', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oyrpNNpf7aP",
        "outputId": "80124487-67d3-4a56-eaac-e430bbaeaf4e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.97      1200\n",
            "           1       0.98      0.97      0.97      1200\n",
            "           2       0.98      0.98      0.98      1200\n",
            "           3       0.97      0.97      0.97      1200\n",
            "\n",
            "    accuracy                           0.97      4800\n",
            "   macro avg       0.97      0.97      0.97      4800\n",
            "weighted avg       0.97      0.97      0.97      4800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = Pipeline([\n",
        "     ('vectorizer_tfidf',TfidfVectorizer()),\n",
        "     ('KNN', KNeighborsClassifier())\n",
        "])\n"
      ],
      "metadata": {
        "id": "kqRUDYKERUiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperate"
      ],
      "metadata": {
        "id": "aODVpLfMSpO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
        "    \"Apple is announcing new iphone tomorrow\",\n",
        "    \"Tesla is announcing new model-3 tomorrow\",\n",
        "    \"Google is announcing new pixel-6 tomorrow\",\n",
        "    \"Microsoft is announcing new surface tomorrow\",\n",
        "    \"Amazon is announcing new eco-dot tomorrow\",\n",
        "    \"I am eating biryani and you are eating grapes\"\n",
        "]"
      ],
      "metadata": {
        "id": "frsQgknZStid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = TfidfVectorizer()\n",
        "v.fit(corpus)\n",
        "transform_output = v.transform(corpus)\n",
        "print(v.vocabulary_)"
      ],
      "metadata": {
        "id": "o3ybdqqhSrQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IDF-Calc\n",
        "all_feature_names = v.get_feature_names_out()\n",
        "for word in all_feature_names:\n",
        "    indx = v.vocabulary_.get(word)\n",
        "    idf_score = v.idf_[indx]\n",
        "    print(f\"{word} : {idf_score}\")"
      ],
      "metadata": {
        "id": "aXFhfwcoSzFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transform_output.toarray())"
      ],
      "metadata": {
        "id": "5ZofF-pnS4BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spacy: GloVe\n",
        "Word Embedding"
      ],
      "metadata": {
        "id": "pYgMNRz2L-E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Fake_Real_Data.csv\")\n",
        "df.columns\n",
        "df.label.value_counts()"
      ],
      "metadata": {
        "id": "4d2hgyFhgh-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.label = df.label.map({'Fake':0,'Real':1})"
      ],
      "metadata": {
        "id": "-_uGYRgZhd7W"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use **glove** embeddings from spacy"
      ],
      "metadata": {
        "id": "kuB002sUj1ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we get vector represetation from spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df['fil_text'] = df.Text.apply(lambda x: nlp(x).vector)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.fil_text.values, df.label, test_size = 0.2)"
      ],
      "metadata": {
        "id": "xQ_0wPyZeCPr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d = np.stack(X_train)\n",
        "X_test_2d = np.stack(X_test)"
      ],
      "metadata": {
        "id": "271LoHP8hoZm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#vector to be in proper range\n",
        "scaler = MinMaxScaler()\n",
        "scaled_train_embed = scaler.fit_transform(X_train_2d)\n",
        "scaled_test_embed = scaler.transform(X_test_2d)"
      ],
      "metadata": {
        "id": "XMhYDBFQjD3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(scaled_train_embed, y_train)\n",
        "y_pred = clf.predict(scaled_test_embed)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "h-favy-dHGEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
        "clf.fit(X_train_2d, y_train)\n",
        "y_pred = clf.predict(X_test_2d)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Y_nogXHKHQzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim: Word2vec\n",
        "\n",
        "sentence representation"
      ],
      "metadata": {
        "id": "gQ6jso-qMEmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GloVe\n",
        "def load_embedding_model():\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.key_to_index.keys()))\n",
        "    return wv_from_bin"
      ],
      "metadata": {
        "id": "5OY6DZjbw7cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ],
      "metadata": {
        "id": "yOFG901RxCyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "wv_great = wv[\"great\"]                          # Vector representation\n",
        "wv.similarity(w1=\"great\", w2=\"good\")\n",
        "wv.most_similar(\"good\")\n",
        "wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
        "wv.doesnt_match([\"facebook\", \"cat\", \"google\", \"microsoft\"])"
      ],
      "metadata": {
        "id": "gJxeJEsdMHfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whole pipeline"
      ],
      "metadata": {
        "id": "p7JKjDHqPREV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"fake_and_real_news.csv\")\n",
        "df['label'].value_counts()\n",
        "df['label_num'] = df['label'].map({'Fake' : 0, 'Real': 1})\n",
        "r2 = wv.get_mean_vector([\"good\", \"great\"],pre_normalize=False)"
      ],
      "metadata": {
        "id": "BDNNWqhjPYJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\") # if this fails then run \"python -m spacy download en_core_web_lg\" to download that model\n",
        "def preprocess_and_vectorize(text):\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "    return wv.get_mean_vector(filtered_tokens)"
      ],
      "metadata": {
        "id": "sUdV2UsgPpIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['vector'] = df['Text'].apply(lambda text: preprocess_and_vectorize(text))"
      ],
      "metadata": {
        "id": "KYs07pU1Pq6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.vector.values,\n",
        "    df.label_num,\n",
        "    test_size=0.2, # 20% samples will go to test dataset\n",
        "    random_state=2022,\n",
        "    stratify=df.label_num\n",
        ")"
      ],
      "metadata": {
        "id": "SuW6N_KhPv-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2d = np.stack(X_train)\n",
        "X_test_2d =  np.stack(X_test)"
      ],
      "metadata": {
        "id": "0tN9GsGxPzf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "clf = GradientBoostingClassifier()\n",
        "clf.fit(X_train_2d, y_train)\n",
        "y_pred = clf.predict(X_test_2d)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xy0uYGXyP0Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions"
      ],
      "metadata": {
        "id": "NZvOEKfXP-3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_news = [\n",
        "    \"Michigan governor denies misleading U.S. House on Flint water (Reuters) - Michigan Governor Rick Snyder denied Thursday that he had misled a U.S. House of Representatives committee last year over testimony on Flintâ€™s water crisis after lawmakers asked if his testimony had been contradicted by a witness in a court hearing. The House Oversight and Government Reform Committee wrote Snyder earlier Thursday asking him about published reports that one of his aides, Harvey Hollins, testified in a court hearing last week in Michigan that he had notified Snyder of an outbreak of Legionnairesâ€™ disease linked to the Flint water crisis in December 2015, rather than 2016 as Snyder had testified. â€œMy testimony was truthful and I stand by it,â€ Snyder told the committee in a letter, adding that his office has provided tens of thousands of pages of records to the committee and would continue to cooperate fully.  Last week, prosecutors in Michigan said Dr. Eden Wells, the stateâ€™s chief medical executive who already faced lesser charges, would become the sixth current or former official to face involuntary manslaughter charges in connection with the crisis. The charges stem from more than 80 cases of Legionnairesâ€™ disease and at least 12 deaths that were believed to be linked to the water in Flint after the city switched its source from Lake Huron to the Flint River in April 2014. Wells was among six current and former Michigan and Flint officials charged in June. The other five, including Michigan Health and Human Services Director Nick Lyon, were charged at the time with involuntary manslaughter\",\n",
        "    \" Sarah Palin Celebrates After White Man Who Pulled Gun On Black Protesters Goes Unpunished (VIDEO) Sarah Palin, one of the nigh-innumerable  deplorables  in Donald Trump s  basket,  almost outdid herself in terms of horribleness on Friday.\"\n",
        "]\n",
        "test_news_vectors = [preprocess_and_vectorize(n) for n in test_news]\n",
        "clf.predict(test_news_vectors)"
      ],
      "metadata": {
        "id": "5e_nDUB0P_8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastText"
      ],
      "metadata": {
        "id": "1m0x1-E7MTTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defaul: skip-gram\n",
        "\n",
        "model = fasttext.train_unsupervised('data', 'cbow')\n"
      ],
      "metadata": {
        "id": "W8Gf9xtBuPa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = fasttext.load_model('C:\\\\Code\\\\nlp-tutorials\\\\downloads\\\\cc.en.300.bin')\n",
        "# model.get_analogies(\"berlin\",\"germany\",\"france\")\n",
        "# model.get_word_vector(\"good\")\n",
        "# model.get_nearest_neighbors(\"halwa\")"
      ],
      "metadata": {
        "id": "yanoi3BWtycD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "tIvIPcB-umWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "df= pd.read_csv(\"ecommerce_dataset.csv\", names=[\"category\", \"description\"], header=None)\n",
        "df.dropna(inplace=True)\n",
        "df.category.replace(\"Clothing & Accessories\", \"Clothing_Accessories\", inplace=True)\n",
        "df['category'] = '__label__' + df['category'].astype(str)\n",
        "df['category_description'] = df['category'] + ' ' + df['description']"
      ],
      "metadata": {
        "id": "aPBvuclmsyOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s\\']',' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "id": "3krStZEVs1_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category_description'] = df['category_description'].map(preprocess)"
      ],
      "metadata": {
        "id": "NdDyumKZtAaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train.to_csv(\"ecommerce.train\", columns=[\"category_description\"], index=False, header=False)\n",
        "test.to_csv(\"ecommerce.test\", columns=[\"category_description\"], index=False, header=False)"
      ],
      "metadata": {
        "id": "yZApnfO9TKpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "model = fasttext.train_supervised(input=\"ecommerce.train\")\n",
        "model.test(\"ecommerce.test\")"
      ],
      "metadata": {
        "id": "0aLE7HqNtgva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(\"wintech assemble desktop pc cpu 500 gb sata hdd 4 gb ram intel c2d processor 3\")\n",
        "# output = __label__electronics"
      ],
      "metadata": {
        "id": "dxnndBM0tkBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors(\"painting\")"
      ],
      "metadata": {
        "id": "C5lIGYyXtmlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-Rd6PDt6nzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means\n"
      ],
      "metadata": {
        "id": "HBlfS3Gr6qXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "WD1eQOHM6uXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"income.csv\")\n",
        "plt.scatter(df.Age,df['Income($)'])"
      ],
      "metadata": {
        "id": "xDzp7yHf60VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df[['Income($)']])\n",
        "df['Income($)'] = scaler.transform(df[['Income($)']])\n",
        "\n",
        "scaler.fit(df[['Age']])\n",
        "df['Age'] = scaler.transform(df[['Age']])"
      ],
      "metadata": {
        "id": "QVPzWPnU7LZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=3)\n",
        "y_predicted = km.fit_predict(df[['Age','Income($)']])\n",
        "df['cluster']=y_predicted\n",
        "km.cluster_centers_\n",
        "\n"
      ],
      "metadata": {
        "id": "auhIyvrf7dZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELBOW Plot"
      ],
      "metadata": {
        "id": "7Q6RzYD67mme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sse = []\n",
        "k_rng = range(1,10)\n",
        "for k in k_rng:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit(df[['Age','Income($)']])\n",
        "    sse.append(km.inertia_)"
      ],
      "metadata": {
        "id": "forRbX237olb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GMM"
      ],
      "metadata": {
        "id": "54KbeY3L8RKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GloVe Stanford a1"
      ],
      "metadata": {
        "id": "qFaxwf16_gMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters"
      ],
      "metadata": {
        "id": "X43PbpMF_j78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "def read_corpus(category=\"crude\"):\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]"
      ],
      "metadata": {
        "id": "1aWusIKY_wCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reuters_corpus = read_corpus()\n",
        "print(reuters_corpus[:3], compact=True, width=100)"
      ],
      "metadata": {
        "id": "my9eUXKs_5Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distinct_words(corpus):\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    corpus_words = {word for doc in corpus for word in doc}\n",
        "    corpus_words = sorted(list(corpus_words))\n",
        "    num_corpus_words = len(corpus_words)\n",
        "    return corpus_words, num_corpus_words"
      ],
      "metadata": {
        "id": "XfMnQ7JU__I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    word2Ind = {}\n",
        "    for index, word in enumerate(words):\n",
        "        word2Ind[word] = index\n",
        "\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence)):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(i + window_size + 1, len(sentence))\n",
        "            words_in_window = sentence[start:end]\n",
        "            curr_word = sentence[i]\n",
        "            for word in words_in_window:\n",
        "                if word!=curr_word:\n",
        "                  M[word2Ind[curr_word], word2Ind[word]] += 1\n",
        "\n",
        "    return M, word2Ind"
      ],
      "metadata": {
        "id": "6A5qhat-ABHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    svd = TruncatedSVD(n_components = k, n_iter = n_iters)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "    return M_reduced"
      ],
      "metadata": {
        "id": "bVvTqRGHAQYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    word_idxs = [word2ind[word] for word in words]\n",
        "    word_vectors = M_reduced[word_idxs]\n",
        "    x_coords = [vec[0] for vec in word_vectors]\n",
        "    y_coords = [vec[1] for vec in word_vectors]\n",
        "    for i, word in enumerate(words):\n",
        "        x = x_coords[i]\n",
        "        y = y_coords[i]\n",
        "        plt.scatter(x, y, marker='x', color='red')\n",
        "        plt.text(x, y, word, fontsize=9)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b8XTrXdMAXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GloVe"
      ],
      "metadata": {
        "id": "v_gMOIBaAcDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding_model():\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.key_to_index.keys()))\n",
        "    return wv_from_bin"
      ],
      "metadata": {
        "id": "ih3ZsMLfAddy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_from_bin = load_embedding_model()"
      ],
      "metadata": {
        "id": "PtQ65yGvAhdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting word vector matrix\n",
        "M = []\n",
        "curInd = 0\n",
        "for w in words:\n",
        "    try:\n",
        "        M.append(wv_from_bin.word_vec(w))\n",
        "        word2ind[w] = curInd\n",
        "        curInd += 1\n",
        "    except KeyError:\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "bJMgyMdtA5Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ],
      "metadata": {
        "id": "MgDHOO1LBLUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}